{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Glue Interactive Sessions Kernel\n",
      "For more information on available magic commands, please type %help in any new cell.\n",
      "\n",
      "Please view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n",
      "Installed kernel version: 1.0.4 \n",
      "Current idle_timeout is None minutes.\n",
      "idle_timeout has been set to 2880 minutes.\n",
      "Setting Glue version to: 4.0\n",
      "Previous worker type: None\n",
      "Setting new worker type to: G.1X\n",
      "Previous number of workers: None\n",
      "Setting new number of workers to: 5\n",
      "Trying to create a Glue session for the kernel.\n",
      "Session Type: glueetl\n",
      "Worker Type: G.1X\n",
      "Number of Workers: 5\n",
      "Session ID: a48c823a-e033-454e-b1fe-f9dd60d27407\n",
      "Applying the following default arguments:\n",
      "--glue_kernel_version 1.0.4\n",
      "--enable-glue-datacatalog true\n",
      "Waiting for session a48c823a-e033-454e-b1fe-f9dd60d27407 to get into ready status...\n",
      "Session a48c823a-e033-454e-b1fe-f9dd60d27407 has been created.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set configuration parameters for the Glue job\n",
    "%idle_timeout 2880     # Set the idle timeout for the Glue job to 2880 seconds (48 minutes)\n",
    "%glue_version 4.0      # Specify the version of AWS Glue to use (version 4.0)\n",
    "%worker_type G.1X      # Define the worker type for the Glue job as G.1X\n",
    "%number_of_workers 5   # Specify the number of workers to use for the Glue job as 5\n",
    "\n",
    "# Import necessary libraries\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from datetime import datetime\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "import boto3\n",
    "from pyspark.sql.functions import *\n",
    "# Create a SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "# Create a GlueContext\n",
    "glueContext = GlueContext(sc)\n",
    "# Create a SparkSession\n",
    "spark = glueContext.spark_session\n",
    "# Create a Glue job\n",
    "job = Job(glueContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define paths to raw data stored in Amazon S3\n",
    "channel_data_path = \"s3://youtube-etl-njk/raw_data/to_processed/channel_data/\"    # Path to channel data\n",
    "video_data_path = \"s3://youtube-etl-njk/raw_data/to_processed/video_data/\"        # Path to video data\n",
    "comments_data_path = \"s3://youtube-etl-njk/raw_data/to_processed/comments_data/\"  # Path to comments data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnalysisException: Path does not exist: s3://youtube-etl-njk/raw_data/to_processed/channel_data\n"
     ]
    }
   ],
   "source": [
    "# Read JSON data from specified paths into Spark DataFrames\n",
    "channel_df = spark.read.json(channel_data_path)    # Read channel data into DataFrame\n",
    "video_df = spark.read.json(video_data_path)        # Read video data into DataFrame\n",
    "comments_df = spark.read.json(comments_data_path)  # Read comments data into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def process_channel_data(channel_df):\n",
    "    # Explode the 'items' array column to create separate rows for each item\n",
    "    Channel_Data = channel_df.withColumn(\"items\", explode('items')).select(\n",
    "        # Select specific columns from the exploded DataFrame and rename them\n",
    "        col(\"items.kind\").alias(\"kind\"),\n",
    "        col(\"items.id\").alias(\"channel_id\"),\n",
    "        col(\"items.contentDetails.relatedPlaylists.uploads\").alias(\"playlist_id\"),\n",
    "        col(\"items.snippet.title\").alias(\"channel_title\"),\n",
    "        # Clean and format the 'channel_description' column\n",
    "        regexp_replace(regexp_replace(col(\"items.snippet.description\"), \",\", \";\"), \"\\s+\", \" \").alias(\"channel_description\"),\n",
    "        col(\"items.snippet.publishedAt\").alias(\"channel_publishedAt\"),\n",
    "        col(\"items.snippet.country\").alias(\"channel_country\"),\n",
    "        col(\"items.statistics.viewCount\").alias(\"channel_viewCount\"),\n",
    "        col(\"items.statistics.subscriberCount\").alias(\"channel_subscriberCount\"),\n",
    "        col(\"items.statistics.videoCount\").alias(\"channel_videoCount\")\n",
    "    ).drop_duplicates(['channel_id'])  # Remove duplicate rows based on 'channel_id'\n",
    "    \n",
    "    # Convert 'channel_publishedAt' column to date format\n",
    "    Channel_Data = Channel_Data.withColumn(\"channel_publishedAt\", to_date(col(\"channel_publishedAt\")))\n",
    "    \n",
    "    # Convert specified columns to integer format\n",
    "    convert_to_int = ['channel_viewCount', 'channel_subscriberCount', 'channel_videoCount']\n",
    "    for column in convert_to_int:\n",
    "        Channel_Data = Channel_Data.withColumn(column, col(column).cast(\"int\"))   \n",
    "    \n",
    "    return Channel_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_data(video_df):\n",
    "    # Explode the 'items' array column to create separate rows for each item\n",
    "    Video_Data = video_df.withColumn(\"items\", explode('items')).select(\n",
    "        # Select specific columns from the exploded DataFrame and rename them\n",
    "        col(\"items.snippet.channelId\").alias(\"channelId\"),\n",
    "        col(\"items.id\").alias(\"video_id\"),\n",
    "        col(\"items.snippet.publishedAt\").alias(\"publishedAt\"),       \n",
    "        # Clean and format the 'title' column\n",
    "        regexp_replace(regexp_replace(col(\"items.snippet.title\"), \",\", \";\"), \"\\s+\", \" \").alias(\"title\"),       \n",
    "        # Clean and format the 'description' column\n",
    "        regexp_replace(regexp_replace(col(\"items.snippet.description\"), \",\", \";\"), \"\\s+\", \" \").alias(\"description\"),      \n",
    "        col(\"items.snippet.categoryId\").alias(\"categoryId\"),\n",
    "        col(\"items.statistics.viewCount\").alias(\"viewCount\"),\n",
    "        col(\"items.statistics.likeCount\").alias(\"likeCount\"),\n",
    "        col(\"items.statistics.commentCount\").alias(\"commentCount\")\n",
    "    ).drop_duplicates(['video_id'])  # Remove duplicate rows based on 'video_id'\n",
    "    \n",
    "    # Convert 'publishedAt' column to date format\n",
    "    Video_Data = Video_Data.withColumn(\"publishedAt\", to_date(col(\"publishedAt\")))\n",
    "    \n",
    "    # Convert specified columns to integer format\n",
    "    convert_to_int = ['categoryId', 'viewCount', 'likeCount', 'commentCount']\n",
    "    for column in convert_to_int:\n",
    "        Video_Data = Video_Data.withColumn(column, col(column).cast(\"int\"))\n",
    "    \n",
    "    return Video_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_comments_data(comments_df):\n",
    "    # Explode the 'items' array column to create separate rows for each item\n",
    "    Comments_Data = comments_df.withColumn(\"items\", explode('items')).select(\n",
    "        # Select specific columns from the exploded DataFrame and rename them\n",
    "        col(\"items.snippet.channelId\").alias(\"channelId\"),\n",
    "        col(\"items.snippet.videoId\").alias(\"videoId\"),\n",
    "        # Clean and format the 'textDisplay' column\n",
    "        regexp_replace(regexp_replace(col(\"items.snippet.topLevelComment.snippet.textDisplay\"), \",\", \";\"), \"\\s+\", \" \").alias(\"textDisplay\"),\n",
    "    ).groupBy(\"channelId\", \"videoId\").agg(collect_list(\"textDisplay\").alias(\"Comments\")).drop_duplicates(['videoId'])\n",
    "    \n",
    "    # Concatenate the 'Comments' column into a single string separated by commas\n",
    "    Comments_Data = Comments_Data.withColumn(\"Comments\", concat_ws(\", \", Comments_Data[\"Comments\"]))\n",
    "    \n",
    "    return Comments_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to process channel data and assign the result to a new variable\n",
    "Channel_Data_Transformed = process_channel_data(channel_df)\n",
    "\n",
    "# Call the function to process video data and assign the result to a new variable\n",
    "Video_Data_Transformed = process_video_data(video_df)\n",
    "\n",
    "# Call the function to process comments data and assign the result to a new variable\n",
    "Comments_Data_Transformed = process_comments_data(comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_s3(df, path_suffix, format_type=\"csv\"):\n",
    "    # Convert DataFrame to DynamicFrame\n",
    "    dynamic_frame = DynamicFrame.fromDF(df, glueContext, \"dynamic_frame\") \n",
    "    \n",
    "    # Write DynamicFrame to S3\n",
    "    glueContext.write_dynamic_frame.from_options(\n",
    "        frame=dynamic_frame,\n",
    "        connection_type=\"s3\",\n",
    "        connection_options={\"path\": f\"s3://youtube-etl-njk/transformed_data/{path_suffix}/\"},\n",
    "        format=format_type\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the write_to_s3() function to write transformed channel data to S3\n",
    "write_to_s3(Channel_Data_Transformed, \"channel_data/channel_transformed_{}\".format(datetime.now().strftime(\"%Y-%m-%d\")), 'csv')\n",
    "\n",
    "# Call the write_to_s3() function to write transformed video data to S3\n",
    "write_to_s3(Video_Data_Transformed, \"video_data/video_transformed_{}\".format(datetime.now().strftime(\"%Y-%m-%d\")), 'csv')\n",
    "\n",
    "# Call the write_to_s3() function to write transformed comments data to S3\n",
    "write_to_s3(Comments_Data_Transformed, \"comments_data/comments_transformed_{}\".format(datetime.now().strftime(\"%Y-%m-%d\")), 'csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_files_to_processed(bucket_name, from_folders, to_folders):\n",
    "    # Create an S3 client\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # Iterate over pairs of 'from_folders' and 'to_folders'\n",
    "    for from_folder, to_folder in zip(from_folders, to_folders):\n",
    "        # List objects in the 'from_folder'\n",
    "        objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=f\"{from_folder}/\")\n",
    "        \n",
    "        # Iterate over each object in the 'from_folder'\n",
    "        for obj in objects.get('Contents', []):\n",
    "            key = obj['Key']  # Get the key (file path) of the object\n",
    "            new_key = key.replace(from_folder, to_folder)  # Generate the new key for the object\n",
    "            \n",
    "            # Copy the object to the 'to_folder'\n",
    "            s3.copy_object(CopySource={'Bucket': bucket_name, 'Key': key}, Bucket=bucket_name, Key=new_key)\n",
    "            \n",
    "            # Delete the object from the 'from_folder'\n",
    "            s3.delete_object(Bucket=bucket_name, Key=key)\n",
    "            \n",
    "            # Print a message indicating the file movement\n",
    "            print(f'Moved file from {key} to {new_key}')\n",
    "\n",
    "# Example usage:\n",
    "bucket_name = 'youtube-etl-njk'\n",
    "from_folders = ['raw_data/to_processed/channel_data', \n",
    "                'raw_data/to_processed/video_data', \n",
    "                'raw_data/to_processed/comments_data']\n",
    "to_folders = ['raw_data/processed/channel_data', \n",
    "              'raw_data/processed/video_data', \n",
    "              'raw_data/processed/comments_data']\n",
    "\n",
    "move_files_to_processed(bucket_name, from_folders, to_folders)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
